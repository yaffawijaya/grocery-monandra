{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93684d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# Common Imports\n",
    "# ------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json # For pretty printing MongoDB documents\n",
    "\n",
    "# ------------------\n",
    "# MongoDB Imports\n",
    "# ------------------\n",
    "from pymongo import MongoClient, ASCENDING, DESCENDING\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "\n",
    "# ------------------\n",
    "# Cassandra Imports\n",
    "# ------------------\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider # If you have auth for Cassandra\n",
    "from cassandra.query import SimpleStatement\n",
    "from cassandra.util import Date\n",
    "from datetime import datetime\n",
    "import uuid # For Cassandra UUID type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b887c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and configuration loaded.\n",
      "MongoDB Connection String loaded from .env\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "MONGODB_CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "MONGODB_DATABASE_NAME = \"grocery_store_db\"\n",
    "CASSANDRA_KEYSPACE = \"day_grocery\"\n",
    "CASSANDRA_CONTACT_POINTS = [\"34.50.95.141\"]\n",
    "CASSANDRA_PORT = 9042\n",
    "\n",
    "EXCEL_FILE_PATH = \"2_synthetic_grocery_data.xlsx\"\n",
    "\n",
    "print(\"Imports and configuration loaded.\")\n",
    "if MONGODB_CONNECTION_STRING:\n",
    "    print(\"MongoDB Connection String loaded from .env\")\n",
    "else:\n",
    "    print(\"ERROR: MongoDB Connection String not found in .env file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0fcfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cabang Data Sample ---\n",
      "  id_cabang              nama_cabang  \\\n",
      "0     CB001  Cabang Banda Aceh Utama   \n",
      "1     CB002      Cabang Padang Utama   \n",
      "\n",
      "                                              lokasi    kontak_cabang  \n",
      "0    Gg. Antapani Lama No. 138, Bengkulu, Jawa Barat  +62-54-779-4795  \n",
      "1  Gang Moch. Toha No. 7, Prabumulih, Sumatera Utara   (010) 934-1587  \n",
      "\n",
      "Loaded 1000 cabang records.\n",
      "\n",
      "--- Karyawan Data Sample ---\n",
      "  id_karyawan          nama_karyawan          jabatan id_cabang\n",
      "0      KR0001     Sutan Nalar Wijaya  Asisten Manajer     CB495\n",
      "1      KR0002  T. Perkasa Mangunsong     Staff Gudang     CB310\n",
      "\n",
      "Loaded 6000 karyawan records.\n",
      "\n",
      "--- Transaksi Harian Data Sample ---\n",
      "                    id_transaksi_harian id_transaksi id_cabang id_karyawan  \\\n",
      "0  3627dbf5-fcdf-4168-bb86-5c654cdb048b     TRX14804     CB621      KR2847   \n",
      "1  7ce15964-2019-4eb2-b00e-3f686cd1a353     TRX27006     CB848      KR1226   \n",
      "\n",
      "      tanggal      nama_barang  qty  harga_barang  total_transaksi  \n",
      "0  2024-05-10  Sosis Ayam 500g    2         30000            60000  \n",
      "1  2024-01-14        Sapu Ijuk    3         15000            45000  \n",
      "\n",
      "Loaded 100000 transaksi harian records.\n",
      "Data types for transaksi_harian:\n",
      "id_transaksi_harian            object\n",
      "id_transaksi                   object\n",
      "id_cabang                      object\n",
      "id_karyawan                    object\n",
      "tanggal                datetime64[ns]\n",
      "nama_barang                    object\n",
      "qty                             int64\n",
      "harga_barang                    int64\n",
      "total_transaksi                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load data from the Excel file generated previously\n",
    "\n",
    "try:\n",
    "    df_cabang = pd.read_excel(EXCEL_FILE_PATH, sheet_name=\"Cabang\")\n",
    "    df_karyawan = pd.read_excel(EXCEL_FILE_PATH, sheet_name=\"Karyawan\")\n",
    "    df_transaksi_harian = pd.read_excel(EXCEL_FILE_PATH, sheet_name=\"Transaksi_Harian\")\n",
    "    \n",
    "    print(\"--- Cabang Data Sample ---\")\n",
    "    print(df_cabang.head(2))\n",
    "    print(f\"\\nLoaded {len(df_cabang)} cabang records.\")\n",
    "    \n",
    "    print(\"\\n--- Karyawan Data Sample ---\")\n",
    "    print(df_karyawan.head(2))\n",
    "    print(f\"\\nLoaded {len(df_karyawan)} karyawan records.\")\n",
    "    \n",
    "    print(\"\\n--- Transaksi Harian Data Sample ---\")\n",
    "    print(df_transaksi_harian.head(2))\n",
    "    # Ensure 'tanggal' is parsed as datetime object for easier conversion later\n",
    "    df_transaksi_harian['tanggal'] = pd.to_datetime(df_transaksi_harian['tanggal'])\n",
    "    print(f\"\\nLoaded {len(df_transaksi_harian)} transaksi harian records.\")\n",
    "    print(f\"Data types for transaksi_harian:\\n{df_transaksi_harian.dtypes}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Excel file '{EXCEL_FILE_PATH}' not found. Please generate it first.\")\n",
    "    # Initialize empty dataframes to prevent errors in later cells if file not found\n",
    "    df_cabang = pd.DataFrame()\n",
    "    df_karyawan = pd.DataFrame()\n",
    "    df_transaksi_harian = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from Excel: {e}\")\n",
    "    df_cabang = pd.DataFrame()\n",
    "    df_karyawan = pd.DataFrame()\n",
    "    df_transaksi_harian = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128017f4",
   "metadata": {},
   "source": [
    "# MongoDB Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55527fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to MongoDB using: mongodb+srv://ilokuda:ilokudat...ter-experiment-yaffa\n",
      "Successfully connected to MongoDB!\n",
      "Selected MongoDB database: 'grocery_store_db'\n"
     ]
    }
   ],
   "source": [
    "# ------------------\n",
    "# MongoDB Operations\n",
    "# ------------------\n",
    "\n",
    "mongo_client = None\n",
    "db = None\n",
    "\n",
    "if MONGODB_CONNECTION_STRING:\n",
    "    try:\n",
    "        print(f\"\\nConnecting to MongoDB using: {MONGODB_CONNECTION_STRING[:30]}...{MONGODB_CONNECTION_STRING[-20:]}\")\n",
    "        mongo_client = MongoClient(MONGODB_CONNECTION_STRING)\n",
    "        mongo_client.admin.command('ismaster') \n",
    "        print(\"Successfully connected to MongoDB!\")\n",
    "        \n",
    "        db = mongo_client[MONGODB_DATABASE_NAME]\n",
    "        print(f\"Selected MongoDB database: '{MONGODB_DATABASE_NAME}'\")\n",
    "        \n",
    "    except ConnectionFailure as e:\n",
    "        print(f\"MongoDB Connection Failed: {e}\")\n",
    "        mongo_client = None\n",
    "        db = None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred with MongoDB connection: {e}\")\n",
    "        mongo_client = None\n",
    "        db = None\n",
    "else:\n",
    "    print(\"MongoDB connection string not available. Skipping MongoDB operations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de663bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted 1000 documents into 'cabang'.\n",
      "Successfully inserted 1000 documents into 'indexed_cabang'.\n",
      "Creating index on 'lokasi' for 'indexed_cabang'...\n",
      "Index 'lokasi_index' created on 'indexed_cabang'.\n",
      "\n",
      "Indexes for 'indexed_cabang':\n",
      "SON([('v', 2), ('key', SON([('_id', 1)])), ('name', '_id_')])\n",
      "SON([('v', 2), ('key', SON([('lokasi', 1)])), ('name', 'lokasi_index')])\n"
     ]
    }
   ],
   "source": [
    "# Check if db object is not None, instead of just 'if db'\n",
    "if db is not None and not df_cabang.empty:\n",
    "    # --- 1. Create 'cabang' collection (id_cabang as _id, no other explicit indexes) ---\n",
    "    try:\n",
    "        cabang_collection_plain = db[\"cabang\"]\n",
    "        if \"cabang\" in db.list_collection_names():\n",
    "            print(\"Dropping existing 'cabang' collection...\")\n",
    "            cabang_collection_plain.drop()\n",
    "        \n",
    "        cabang_data_mongo = df_cabang.to_dict(orient=\"records\")\n",
    "        for record in cabang_data_mongo: # Use id_cabang as _id\n",
    "            record[\"_id\"] = record[\"id_cabang\"]\n",
    "            \n",
    "        result_plain = cabang_collection_plain.insert_many(cabang_data_mongo)\n",
    "        print(f\"Successfully inserted {len(result_plain.inserted_ids)} documents into 'cabang'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during 'cabang' ingestion: {e}\")\n",
    "\n",
    "    # --- 2. Create 'indexed_cabang' collection (id_cabang as _id, index on 'lokasi') ---\n",
    "    try:\n",
    "        cabang_collection_indexed = db[\"indexed_cabang\"]\n",
    "        if \"indexed_cabang\" in db.list_collection_names():\n",
    "            print(\"Dropping existing 'indexed_cabang' collection...\")\n",
    "            cabang_collection_indexed.drop()\n",
    "            \n",
    "        result_indexed = cabang_collection_indexed.insert_many(cabang_data_mongo) \n",
    "        print(f\"Successfully inserted {len(result_indexed.inserted_ids)} documents into 'indexed_cabang'.\")\n",
    "        \n",
    "        print(\"Creating index on 'lokasi' for 'indexed_cabang'...\")\n",
    "        cabang_collection_indexed.create_index([(\"lokasi\", ASCENDING)], name=\"lokasi_index\")\n",
    "        print(\"Index 'lokasi_index' created on 'indexed_cabang'.\")\n",
    "\n",
    "        print(\"\\nIndexes for 'indexed_cabang':\")\n",
    "        for index in cabang_collection_indexed.list_indexes():\n",
    "            print(index)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during 'indexed_cabang' ingestion/indexing: {e}\")\n",
    "elif df_cabang.empty:\n",
    "    print(\"Cabang DataFrame is empty. Skipping MongoDB ingestion for Cabang.\")\n",
    "else: # This means db is None\n",
    "    print(\"MongoDB connection not established (db is None). Skipping Cabang ingestion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83fd769a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted 6000 documents into 'karyawan'.\n",
      "Successfully inserted 6000 documents into 'indexed_karyawan'.\n",
      "Creating compound index on ('nama_karyawan', 'jabatan') for 'indexed_karyawan'...\n",
      "Index 'nama_jabatan_compound_index' created on 'indexed_karyawan'.\n",
      "\n",
      "Indexes for 'indexed_karyawan':\n",
      "SON([('v', 2), ('key', SON([('_id', 1)])), ('name', '_id_')])\n",
      "SON([('v', 2), ('key', SON([('nama_karyawan', 1), ('jabatan', 1)])), ('name', 'nama_jabatan_compound_index')])\n"
     ]
    }
   ],
   "source": [
    "if db is not None and not df_karyawan.empty:\n",
    "    # --- 1. Create 'karyawan' collection (id_karyawan as _id, no other explicit indexes) ---\n",
    "    try:\n",
    "        karyawan_collection_plain = db[\"karyawan\"]\n",
    "        if \"karyawan\" in db.list_collection_names():\n",
    "            print(\"Dropping existing 'karyawan' collection...\")\n",
    "            karyawan_collection_plain.drop()\n",
    "\n",
    "        karyawan_data_mongo = df_karyawan.to_dict(orient=\"records\")\n",
    "        for record in karyawan_data_mongo: # Use id_karyawan as _id\n",
    "            record[\"_id\"] = record[\"id_karyawan\"]\n",
    "            \n",
    "        result_plain = karyawan_collection_plain.insert_many(karyawan_data_mongo)\n",
    "        print(f\"Successfully inserted {len(result_plain.inserted_ids)} documents into 'karyawan'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during 'karyawan' ingestion: {e}\")\n",
    "\n",
    "    # --- 2. Create 'indexed_karyawan' collection (id_karyawan as _id, compound index) ---\n",
    "    try:\n",
    "        karyawan_collection_indexed = db[\"indexed_karyawan\"]\n",
    "        if \"indexed_karyawan\" in db.list_collection_names():\n",
    "            print(\"Dropping existing 'indexed_karyawan' collection...\")\n",
    "            karyawan_collection_indexed.drop()\n",
    "        \n",
    "        # Data is the same (karyawan_data_mongo already has _id set)\n",
    "        result_indexed = karyawan_collection_indexed.insert_many(karyawan_data_mongo)\n",
    "        print(f\"Successfully inserted {len(result_indexed.inserted_ids)} documents into 'indexed_karyawan'.\")\n",
    "        \n",
    "        # Create compound index on (nama_karyawan, jabatan)\n",
    "        print(\"Creating compound index on ('nama_karyawan', 'jabatan') for 'indexed_karyawan'...\")\n",
    "        karyawan_collection_indexed.create_index(\n",
    "            [(\"nama_karyawan\", ASCENDING), (\"jabatan\", ASCENDING)], \n",
    "            name=\"nama_jabatan_compound_index\"\n",
    "        )\n",
    "        print(\"Index 'nama_jabatan_compound_index' created on 'indexed_karyawan'.\")\n",
    "\n",
    "        # Verify by listing indexes\n",
    "        print(\"\\nIndexes for 'indexed_karyawan':\")\n",
    "        for index in karyawan_collection_indexed.list_indexes():\n",
    "            print(index)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during 'indexed_karyawan' ingestion/indexing: {e}\")\n",
    "elif df_karyawan.empty:\n",
    "    print(\"Karyawan DataFrame is empty. Skipping MongoDB ingestion for Karyawan.\")\n",
    "else:\n",
    "    print(\"MongoDB connection not established. Skipping Karyawan ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f1eb3",
   "metadata": {},
   "source": [
    "# Cassandra Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd30da6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connecting to Cassandra cluster at ['34.50.95.141']:9042...\n",
      "Creating keyspace 'day_grocery' if it doesn't exist...\n",
      "Keyspace 'day_grocery' ensured.\n",
      "Connecting to keyspace 'day_grocery'...\n",
      "Successfully connected to Cassandra and keyspace.\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Cassandra Operations\n",
    "# --------------------\n",
    "cassandra_cluster = None\n",
    "cassandra_session = None\n",
    "\n",
    "try:\n",
    "    print(f\"\\nConnecting to Cassandra cluster at {CASSANDRA_CONTACT_POINTS}:{CASSANDRA_PORT}...\")\n",
    "    cassandra_cluster = Cluster(contact_points=CASSANDRA_CONTACT_POINTS, port=CASSANDRA_PORT)\n",
    "    temp_session = cassandra_cluster.connect()\n",
    "    \n",
    "    print(f\"Creating keyspace '{CASSANDRA_KEYSPACE}' if it doesn't exist...\")\n",
    "    temp_session.execute(f\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS {CASSANDRA_KEYSPACE}\n",
    "        WITH replication = {{ 'class': 'SimpleStrategy', 'replication_factor': '1' }}\n",
    "    \"\"\")\n",
    "    print(f\"Keyspace '{CASSANDRA_KEYSPACE}' ensured.\")\n",
    "    temp_session.shutdown()\n",
    "\n",
    "    print(f\"Connecting to keyspace '{CASSANDRA_KEYSPACE}'...\")\n",
    "    cassandra_session = cassandra_cluster.connect(CASSANDRA_KEYSPACE)\n",
    "    print(\"Successfully connected to Cassandra and keyspace.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Cassandra connection or keyspace creation failed: {e}\")\n",
    "    if cassandra_cluster:\n",
    "        cassandra_cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3363436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table 'transaksi_harian' in keyspace 'day_grocery'...\n",
      "Table 'transaksi_harian' created successfully or already exists.\n",
      "Creating table 'indexed_transaksi_harian' in keyspace 'day_grocery'...\n",
      "Table 'indexed_transaksi_harian' created successfully or already exists.\n"
     ]
    }
   ],
   "source": [
    "if cassandra_session:\n",
    "    # --- 1. Create 'transaksi_harian' table ---\n",
    "    try:\n",
    "        table_name_plain = \"transaksi_harian\"\n",
    "        print(f\"Creating table '{table_name_plain}' in keyspace '{CASSANDRA_KEYSPACE}'...\")\n",
    "        \n",
    "        # cassandra_session.execute(f\"DROP TABLE IF EXISTS {table_name_plain}\")\n",
    "\n",
    "        create_table_plain_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name_plain} (\n",
    "            id_transaksi_harian UUID PRIMARY KEY,\n",
    "            id_transaksi TEXT,\n",
    "            id_cabang TEXT,\n",
    "            id_karyawan TEXT,\n",
    "            tanggal DATE,\n",
    "            nama_barang TEXT,\n",
    "            qty INT,\n",
    "            harga_barang INT,\n",
    "            total_transaksi INT\n",
    "        );\n",
    "        \"\"\"\n",
    "        cassandra_session.execute(create_table_plain_query)\n",
    "        print(f\"Table '{table_name_plain}' created successfully or already exists.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Cassandra table '{table_name_plain}': {e}\")\n",
    "\n",
    "    # --- 2. Create 'indexed_transaksi_harian' table ---\n",
    "    # Primary Key: Partition by (id_cabang, id_karyawan, nama_barang), Clustered by tanggal for ordering\n",
    "    # id_transaksi_harian is added to clustering key to ensure uniqueness if other components are not unique per row\n",
    "    try:\n",
    "        table_name_indexed = \"indexed_transaksi_harian\"\n",
    "        print(f\"Creating table '{table_name_indexed}' in keyspace '{CASSANDRA_KEYSPACE}'...\")\n",
    "\n",
    "        # cassandra_session.execute(f\"DROP TABLE IF EXISTS {table_name_indexed}\")\n",
    "        \n",
    "        create_table_indexed_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name_indexed} (\n",
    "            id_cabang TEXT,\n",
    "            id_karyawan TEXT,\n",
    "            nama_barang TEXT,\n",
    "            tanggal DATE,\n",
    "            id_transaksi_harian UUID, \n",
    "            id_transaksi TEXT,\n",
    "            qty INT,\n",
    "            harga_barang INT,\n",
    "            total_transaksi INT,\n",
    "            PRIMARY KEY ((id_cabang, id_karyawan, nama_barang), tanggal, id_transaksi_harian)\n",
    "        ) WITH CLUSTERING ORDER BY (tanggal ASC, id_transaksi_harian ASC);\n",
    "        \"\"\"\n",
    "        # Note: Partition key fields (id_cabang, id_karyawan, nama_barang) must be listed first.\n",
    "        # Other fields become regular columns.\n",
    "        cassandra_session.execute(create_table_indexed_query)\n",
    "        print(f\"Table '{table_name_indexed}' created successfully or already exists.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Cassandra table '{table_name_indexed}': {e}\")\n",
    "else:\n",
    "    print(\"Cassandra session not established. Skipping table creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dc840",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0899ed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records = len(df_transaksi_harian)\n",
    "all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ad5fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing to insert all 100000 records into Cassandra tables 'transaksi_harian' and 'indexed_transaksi_harian'...\n",
      "Processed 1000/100000 records. Time Elapsed: 0m 39s. ETA: 65m 21s\n",
      "Processed 2000/100000 records. Time Elapsed: 1m 17s. ETA: 63m 20s\n",
      "Processed 3000/100000 records. Time Elapsed: 1m 54s. ETA: 61m 31s\n",
      "Processed 4000/100000 records. Time Elapsed: 2m 32s. ETA: 60m 52s\n",
      "Processed 5000/100000 records. Time Elapsed: 3m 8s. ETA: 59m 48s\n",
      "Processed 6000/100000 records. Time Elapsed: 3m 44s. ETA: 58m 30s\n",
      "Processed 7000/100000 records. Time Elapsed: 4m 20s. ETA: 57m 39s\n",
      "Processed 8000/100000 records. Time Elapsed: 4m 57s. ETA: 56m 55s\n",
      "Processed 9000/100000 records. Time Elapsed: 5m 34s. ETA: 56m 20s\n",
      "Processed 10000/100000 records. Time Elapsed: 6m 9s. ETA: 55m 27s\n",
      "Processed 11000/100000 records. Time Elapsed: 6m 46s. ETA: 54m 50s\n",
      "Processed 12000/100000 records. Time Elapsed: 7m 23s. ETA: 54m 14s\n",
      "Processed 13000/100000 records. Time Elapsed: 8m 0s. ETA: 53m 38s\n",
      "Processed 14000/100000 records. Time Elapsed: 8m 37s. ETA: 53m 0s\n",
      "Processed 15000/100000 records. Time Elapsed: 9m 15s. ETA: 52m 28s\n",
      "Processed 16000/100000 records. Time Elapsed: 9m 51s. ETA: 51m 46s\n",
      "Processed 17000/100000 records. Time Elapsed: 10m 27s. ETA: 51m 3s\n",
      "Processed 18000/100000 records. Time Elapsed: 11m 4s. ETA: 50m 25s\n",
      "Processed 19000/100000 records. Time Elapsed: 11m 39s. ETA: 49m 41s\n",
      "Processed 20000/100000 records. Time Elapsed: 12m 17s. ETA: 49m 8s\n",
      "Processed 21000/100000 records. Time Elapsed: 12m 54s. ETA: 48m 32s\n",
      "Processed 22000/100000 records. Time Elapsed: 13m 30s. ETA: 47m 53s\n",
      "Processed 23000/100000 records. Time Elapsed: 14m 7s. ETA: 47m 17s\n",
      "Processed 24000/100000 records. Time Elapsed: 14m 47s. ETA: 46m 49s\n",
      "Processed 25000/100000 records. Time Elapsed: 15m 25s. ETA: 46m 17s\n",
      "Processed 26000/100000 records. Time Elapsed: 16m 3s. ETA: 45m 42s\n",
      "Processed 27000/100000 records. Time Elapsed: 16m 40s. ETA: 45m 6s\n",
      "Processed 28000/100000 records. Time Elapsed: 17m 18s. ETA: 44m 29s\n",
      "Processed 29000/100000 records. Time Elapsed: 17m 55s. ETA: 43m 52s\n",
      "Processed 30000/100000 records. Time Elapsed: 18m 32s. ETA: 43m 16s\n",
      "Processed 31000/100000 records. Time Elapsed: 19m 9s. ETA: 42m 38s\n",
      "Processed 32000/100000 records. Time Elapsed: 19m 50s. ETA: 42m 10s\n",
      "Processed 33000/100000 records. Time Elapsed: 20m 26s. ETA: 41m 29s\n",
      "Processed 34000/100000 records. Time Elapsed: 21m 4s. ETA: 40m 53s\n",
      "Processed 35000/100000 records. Time Elapsed: 21m 41s. ETA: 40m 16s\n",
      "Processed 36000/100000 records. Time Elapsed: 22m 17s. ETA: 39m 37s\n",
      "Processed 37000/100000 records. Time Elapsed: 22m 55s. ETA: 39m 2s\n",
      "Processed 38000/100000 records. Time Elapsed: 23m 35s. ETA: 38m 28s\n",
      "Processed 39000/100000 records. Time Elapsed: 24m 11s. ETA: 37m 50s\n",
      "Processed 40000/100000 records. Time Elapsed: 24m 48s. ETA: 37m 12s\n",
      "Processed 41000/100000 records. Time Elapsed: 25m 25s. ETA: 36m 35s\n",
      "Processed 42000/100000 records. Time Elapsed: 26m 3s. ETA: 35m 58s\n",
      "Processed 43000/100000 records. Time Elapsed: 26m 49s. ETA: 35m 33s\n",
      "Processed 44000/100000 records. Time Elapsed: 27m 27s. ETA: 34m 56s\n",
      "Processed 45000/100000 records. Time Elapsed: 28m 3s. ETA: 34m 17s\n",
      "Processed 46000/100000 records. Time Elapsed: 28m 39s. ETA: 33m 38s\n",
      "Processed 47000/100000 records. Time Elapsed: 29m 16s. ETA: 33m 0s\n",
      "Processed 48000/100000 records. Time Elapsed: 29m 56s. ETA: 32m 25s\n",
      "Processed 49000/100000 records. Time Elapsed: 30m 33s. ETA: 31m 48s\n",
      "Processed 50000/100000 records. Time Elapsed: 31m 12s. ETA: 31m 12s\n",
      "Processed 51000/100000 records. Time Elapsed: 31m 52s. ETA: 30m 37s\n",
      "Processed 52000/100000 records. Time Elapsed: 32m 34s. ETA: 30m 3s\n",
      "Processed 53000/100000 records. Time Elapsed: 33m 13s. ETA: 29m 28s\n",
      "Processed 54000/100000 records. Time Elapsed: 33m 52s. ETA: 28m 51s\n",
      "Processed 55000/100000 records. Time Elapsed: 34m 29s. ETA: 28m 12s\n",
      "Processed 56000/100000 records. Time Elapsed: 35m 6s. ETA: 27m 34s\n",
      "Processed 57000/100000 records. Time Elapsed: 35m 41s. ETA: 26m 55s\n",
      "Processed 58000/100000 records. Time Elapsed: 36m 16s. ETA: 26m 16s\n",
      "Processed 59000/100000 records. Time Elapsed: 36m 52s. ETA: 25m 37s\n",
      "Processed 60000/100000 records. Time Elapsed: 37m 29s. ETA: 24m 59s\n",
      "Processed 61000/100000 records. Time Elapsed: 38m 3s. ETA: 24m 19s\n",
      "Processed 62000/100000 records. Time Elapsed: 38m 41s. ETA: 23m 42s\n",
      "Processed 63000/100000 records. Time Elapsed: 39m 18s. ETA: 23m 5s\n",
      "Processed 64000/100000 records. Time Elapsed: 39m 55s. ETA: 22m 27s\n",
      "Processed 65000/100000 records. Time Elapsed: 40m 31s. ETA: 21m 49s\n",
      "Processed 66000/100000 records. Time Elapsed: 41m 8s. ETA: 21m 11s\n",
      "Processed 67000/100000 records. Time Elapsed: 41m 42s. ETA: 20m 32s\n",
      "Processed 68000/100000 records. Time Elapsed: 42m 18s. ETA: 19m 54s\n",
      "Processed 69000/100000 records. Time Elapsed: 42m 56s. ETA: 19m 17s\n",
      "Processed 70000/100000 records. Time Elapsed: 43m 32s. ETA: 18m 39s\n",
      "Processed 71000/100000 records. Time Elapsed: 44m 8s. ETA: 18m 1s\n",
      "Processed 72000/100000 records. Time Elapsed: 44m 44s. ETA: 17m 24s\n",
      "Processed 73000/100000 records. Time Elapsed: 45m 31s. ETA: 16m 50s\n",
      "Processed 74000/100000 records. Time Elapsed: 46m 7s. ETA: 16m 12s\n",
      "Processed 75000/100000 records. Time Elapsed: 46m 44s. ETA: 15m 34s\n",
      "Processed 76000/100000 records. Time Elapsed: 47m 21s. ETA: 14m 57s\n",
      "Processed 77000/100000 records. Time Elapsed: 48m 2s. ETA: 14m 20s\n",
      "Processed 78000/100000 records. Time Elapsed: 48m 39s. ETA: 13m 43s\n",
      "Processed 79000/100000 records. Time Elapsed: 49m 16s. ETA: 13m 5s\n",
      "Processed 80000/100000 records. Time Elapsed: 49m 51s. ETA: 12m 27s\n",
      "Processed 81000/100000 records. Time Elapsed: 50m 31s. ETA: 11m 51s\n",
      "Processed 82000/100000 records. Time Elapsed: 51m 10s. ETA: 11m 14s\n",
      "Processed 83000/100000 records. Time Elapsed: 51m 47s. ETA: 10m 36s\n",
      "Processed 84000/100000 records. Time Elapsed: 52m 25s. ETA: 9m 59s\n",
      "Processed 85000/100000 records. Time Elapsed: 53m 2s. ETA: 9m 21s\n",
      "Processed 86000/100000 records. Time Elapsed: 53m 36s. ETA: 8m 43s\n",
      "Processed 87000/100000 records. Time Elapsed: 54m 9s. ETA: 8m 5s\n",
      "Processed 88000/100000 records. Time Elapsed: 54m 45s. ETA: 7m 28s\n",
      "Processed 89000/100000 records. Time Elapsed: 55m 21s. ETA: 6m 50s\n",
      "Processed 90000/100000 records. Time Elapsed: 55m 57s. ETA: 6m 13s\n",
      "Processed 91000/100000 records. Time Elapsed: 56m 34s. ETA: 5m 35s\n",
      "Processed 92000/100000 records. Time Elapsed: 57m 11s. ETA: 4m 58s\n",
      "Processed 93000/100000 records. Time Elapsed: 57m 47s. ETA: 4m 20s\n",
      "Processed 94000/100000 records. Time Elapsed: 58m 22s. ETA: 3m 43s\n",
      "Processed 95000/100000 records. Time Elapsed: 58m 57s. ETA: 3m 6s\n",
      "Processed 96000/100000 records. Time Elapsed: 59m 31s. ETA: 2m 28s\n",
      "Processed 97000/100000 records. Time Elapsed: 60m 7s. ETA: 1m 51s\n",
      "Processed 98000/100000 records. Time Elapsed: 60m 42s. ETA: 1m 14s\n",
      "Processed 99000/100000 records. Time Elapsed: 61m 18s. ETA: 0m 37s\n",
      "Processed 100000/100000 records. Time Elapsed: 61m 53s. ETA: Done!\n",
      "\n",
      "--- Cassandra Ingestion Summary ---\n",
      "Attempted to process: 100000 records.\n",
      "Successfully inserted into both tables: 100000 records.\n",
      "Failed to insert: 0 records.\n",
      "Total ingestion time: 61m 53s (3713.67 seconds).\n",
      "\n",
      "Sample data from Cassandra table 'transaksi_harian' (limit 1):\n",
      "Row(id_transaksi_harian=UUID('c2774d0a-68b1-41c9-a09c-d39cddd46e85'), harga_barang=18000, id_cabang='CB208', id_karyawan='KR3978', id_transaksi='TRX32675', nama_barang='Obat Nyamuk Semprot', qty=1, tanggal=Date(19795), total_transaksi=18000)\n",
      "\n",
      "Sample data from Cassandra table 'indexed_transaksi_harian' (limit 1):\n",
      "Row(id_cabang='CB962', id_karyawan='KR1215', nama_barang='Kopi Instan Botol 240ml', tanggal=Date(19725), id_transaksi_harian=UUID('26907c8c-e24e-4b40-a0a6-795d74a68432'), harga_barang=9000, id_transaksi='TRX03886', qty=3, total_transaksi=27000)\n"
     ]
    }
   ],
   "source": [
    "import time # Make sure time is imported\n",
    "import uuid # Make sure uuid is imported\n",
    "# Assuming pandas (pd) is already imported and df_transaksi_harian, cassandra_session exist.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the maximum number of records for this run. \n",
    "# For example, to process all records:\n",
    "MAX_CASSANDRA_RECORDS_TO_INGEST = all_records\n",
    "# Or for a specific limit, e.g., 3000:\n",
    "# MAX_CASSANDRA_RECORDS_TO_INGEST = 3000 \n",
    "# Or, if you have a variable 'all_records' that holds the desired count:\n",
    "# MAX_CASSANDRA_RECORDS_TO_INGEST = all_records \n",
    "\n",
    "\n",
    "if cassandra_session is not None and not df_transaksi_harian.empty:\n",
    "    table_plain = \"transaksi_harian\"\n",
    "    table_indexed = \"indexed_transaksi_harian\"\n",
    "    \n",
    "    num_total_records_in_df = len(df_transaksi_harian)\n",
    "    \n",
    "    if num_total_records_in_df == 0:\n",
    "        print(\"Transaksi Harian DataFrame is empty. Nothing to ingest.\")\n",
    "    else:\n",
    "        # Determine actual records to process based on the limit\n",
    "        if MAX_CASSANDRA_RECORDS_TO_INGEST >= num_total_records_in_df:\n",
    "            df_ingest_subset = df_transaksi_harian\n",
    "            actual_records_to_ingest = num_total_records_in_df\n",
    "            print(f\"\\nPreparing to insert all {actual_records_to_ingest} records into Cassandra tables '{table_plain}' and '{table_indexed}'...\")\n",
    "        else:\n",
    "            df_ingest_subset = df_transaksi_harian.head(MAX_CASSANDRA_RECORDS_TO_INGEST)\n",
    "            actual_records_to_ingest = MAX_CASSANDRA_RECORDS_TO_INGEST\n",
    "            print(f\"\\nPreparing to insert {actual_records_to_ingest} records (limited from {num_total_records_in_df} total) into Cassandra tables '{table_plain}' and '{table_indexed}'...\")\n",
    "\n",
    "        insert_cql_plain = f\"\"\"\n",
    "        INSERT INTO {table_plain} (id_transaksi_harian, id_transaksi, id_cabang, id_karyawan, tanggal, \n",
    "            nama_barang, qty, harga_barang, total_transaksi) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        prepared_plain = cassandra_session.prepare(insert_cql_plain)\n",
    "        \n",
    "        insert_cql_indexed = f\"\"\"\n",
    "        INSERT INTO {table_indexed} (id_cabang, id_karyawan, nama_barang, tanggal, id_transaksi_harian, \n",
    "            id_transaksi, qty, harga_barang, total_transaksi) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        prepared_indexed = cassandra_session.prepare(insert_cql_indexed)\n",
    "        \n",
    "        inserted_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        start_loop_time = time.perf_counter() # Start timer before the loop\n",
    "\n",
    "        for index, row in df_ingest_subset.iterrows():\n",
    "            try:\n",
    "                id_harian_uuid = uuid.UUID(str(row['id_transaksi_harian']))\n",
    "                tanggal_date_obj = row['tanggal'].date()\n",
    "\n",
    "                cassandra_session.execute(prepared_plain, (\n",
    "                    id_harian_uuid, row['id_transaksi'], row['id_cabang'], row['id_karyawan'],\n",
    "                    tanggal_date_obj, row['nama_barang'], int(row['qty']), \n",
    "                    int(row['harga_barang']), int(row['total_transaksi'])\n",
    "                ))\n",
    "                \n",
    "                cassandra_session.execute(prepared_indexed, (\n",
    "                    row['id_cabang'], row['id_karyawan'], row['nama_barang'], tanggal_date_obj, \n",
    "                    id_harian_uuid, row['id_transaksi'], int(row['qty']), \n",
    "                    int(row['harga_barang']), int(row['total_transaksi'])\n",
    "                ))\n",
    "                \n",
    "                inserted_count += 1\n",
    "                \n",
    "                # --- ETA Calculation and Progress Print ---\n",
    "                if inserted_count % 1000 == 0 or inserted_count == actual_records_to_ingest:\n",
    "                    current_loop_time = time.perf_counter()\n",
    "                    elapsed_time = current_loop_time - start_loop_time\n",
    "                    eta_str = \"Calculating...\"\n",
    "                    \n",
    "                    if elapsed_time > 0 and inserted_count > 0: # Avoid division by zero and ensure some progress\n",
    "                        speed = inserted_count / elapsed_time  # records per second\n",
    "                        remaining_records = actual_records_to_ingest - inserted_count\n",
    "                        if speed > 0 and remaining_records > 0:\n",
    "                            eta_seconds = remaining_records / speed\n",
    "                            eta_minutes = int(eta_seconds // 60)\n",
    "                            eta_remainder_seconds = int(eta_seconds % 60)\n",
    "                            eta_str = f\"{eta_minutes}m {eta_remainder_seconds}s\"\n",
    "                        elif remaining_records == 0:\n",
    "                            eta_str = \"Done!\"\n",
    "                        else:\n",
    "                            eta_str = \"Estimating...\"\n",
    "                    \n",
    "                    total_elapsed_minutes = int(elapsed_time // 60)\n",
    "                    total_elapsed_seconds = int(elapsed_time % 60)\n",
    "                    \n",
    "                    print(f\"Processed {inserted_count}/{actual_records_to_ingest} records. \"\n",
    "                          f\"Time Elapsed: {total_elapsed_minutes}m {total_elapsed_seconds}s. ETA: {eta_str}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to insert row {index} (id_transaksi_harian: {row.get('id_transaksi_harian', 'N/A')}) into Cassandra: {row.to_dict()}\")\n",
    "                print(f\"Error: {e}\")\n",
    "                failed_count += 1\n",
    "                if failed_count > 20:\n",
    "                    print(\"Too many errors, stopping Cassandra ingestion.\")\n",
    "                    break\n",
    "        \n",
    "        end_loop_time = time.perf_counter()\n",
    "        total_ingestion_time = end_loop_time - start_loop_time\n",
    "        total_ingestion_minutes = int(total_ingestion_time // 60)\n",
    "        total_ingestion_remainder_seconds = int(total_ingestion_time % 60)\n",
    "\n",
    "        print(f\"\\n--- Cassandra Ingestion Summary ---\")\n",
    "        print(f\"Attempted to process: {actual_records_to_ingest} records.\")\n",
    "        print(f\"Successfully inserted into both tables: {inserted_count} records.\")\n",
    "        print(f\"Failed to insert: {failed_count} records.\")\n",
    "        print(f\"Total ingestion time: {total_ingestion_minutes}m {total_ingestion_remainder_seconds}s ({total_ingestion_time:.2f} seconds).\")\n",
    "\n",
    "        if inserted_count > 0:\n",
    "            print(f\"\\nSample data from Cassandra table '{table_plain}' (limit 1):\")\n",
    "            for r_plain in cassandra_session.execute(f\"SELECT * FROM {table_plain} LIMIT 1\"): print(r_plain)\n",
    "            print(f\"\\nSample data from Cassandra table '{table_indexed}' (limit 1):\")\n",
    "            for r_indexed in cassandra_session.execute(f\"SELECT * FROM {table_indexed} LIMIT 1\"): print(r_indexed)\n",
    "            \n",
    "elif df_transaksi_harian.empty:\n",
    "    print(\"Transaksi Harian DataFrame is empty. Skipping Cassandra ingestion.\")\n",
    "else: # cassandra_session is None\n",
    "    print(\"Cassandra session not established. Skipping Transaksi_Harian ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489e7d9",
   "metadata": {},
   "source": [
    "# Close Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up connections\n",
    "if mongo_client:\n",
    "    print(\"\\nClosing MongoDB connection...\")\n",
    "    mongo_client.close()\n",
    "    print(\"MongoDB connection closed.\")\n",
    "\n",
    "print(\"\\Alhamdulillah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6ae72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mongo_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Clean up connections\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmongo_client\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mClosing MongoDB connection...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     mongo_client.close()\n",
      "\u001b[31mNameError\u001b[39m: name 'mongo_client' is not defined"
     ]
    }
   ],
   "source": [
    "if cassandra_cluster:\n",
    "    print(\"\\nClosing Cassandra connection...\")\n",
    "    cassandra_cluster.shutdown() \n",
    "    print(\"Cassandra connection closed.\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n",
    "# alhamdulillah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c0cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monandra311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
